

import os
import zipfile
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from keras.preprocessing import image
from google.colab import drive, files
import requests

# Mount Google Drive
drive.mount('/content/drive')
drive_path = '/content/drive/MyDrive/EuroSAT_TF'
os.makedirs(drive_path, exist_ok=True)

# Step 1: Download and extract EuroSAT
url = "http://madm.dfki.de/files/sentinel/EuroSAT.zip"
zip_path = "/content/EuroSAT.zip"
extract_path = "/content/EuroSAT"

if not os.path.exists(zip_path):
    r = requests.get(url)
    with open(zip_path, 'wb') as f:
        f.write(r.content)

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# Step 2: Filter only selected 4 classes
selected_classes = ['Forest', 'Highway', 'River', 'Residential']
source_base = os.path.join(extract_path, '2750')
filtered_data = '/content/filtered_data'
os.makedirs(filtered_data, exist_ok=True)

for cls in selected_classes:
    src = os.path.join(source_base, cls)
    dst = os.path.join(filtered_data, cls)
    os.makedirs(dst, exist_ok=True)
    files_to_copy = os.listdir(src)[:1000]
    for f in files_to_copy:
        tf.io.gfile.copy(os.path.join(src, f), os.path.join(dst, f), overwrite=True)

# Step 3: Load data
BATCH_SIZE = 32
IMG_SIZE = (224, 224)
EPOCHS = 10

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    filtered_data,
    validation_split=0.2,
    subset="training",
    seed=123,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE
)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    filtered_data,
    validation_split=0.2,
    subset="validation",
    seed=123,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE
)

class_names = train_ds.class_names
print("Classes:", class_names)

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)

# Step 4: Build model
base_model = tf.keras.applications.EfficientNetB0(input_shape=IMG_SIZE + (3,), include_top=False, weights='imagenet')
base_model.trainable = False

model = tf.keras.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(len(class_names), activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)

# Step 5: Save model & accuracy plot
model.save(os.path.join(drive_path, 'eurosat_tf_model.h5'))

plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Accuracy Curve")
plt.savefig(os.path.join(drive_path, 'accuracy_plot.png'))
plt.show()
plt.close()

# Step 6: Evaluation
y_true, y_pred = [], []
for images, labels in val_ds:
    preds = model.predict(images)
    y_true.extend(labels.numpy())
    y_pred.extend(np.argmax(preds, axis=1))

cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.savefig(os.path.join(drive_path, 'confusion_matrix.png'))
plt.show()  # âœ… Show confusion matrix while running
plt.close()

report = classification_report(y_true, y_pred, target_names=class_names)
print(report)
with open(os.path.join(drive_path, 'classification_report.txt'), 'w') as f:
    f.write(report)

# Step 7: Predict on uploaded image
uploaded = files.upload()
for fn in uploaded.keys():
    img = image.load_img(fn, target_size=IMG_SIZE)
    img_array = image.img_to_array(img)
    img_array = tf.expand_dims(img_array, 0)
    img_array = tf.keras.applications.efficientnet.preprocess_input(img_array)
    preds = model.predict(img_array)
    predicted_label = class_names[np.argmax(preds)]
    print(f"Prediction: {predicted_label} ({100*np.max(preds):.2f}%)")
